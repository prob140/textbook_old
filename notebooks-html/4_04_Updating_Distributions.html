<div id="ipython-notebook">
            <a class="interact-button" href="http://prob140.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/prob140/materials&branch=gh-pages&subPath=textbook/4_04_Updating_Distributions.ipynb">Interact</a>
            
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Updating-Distributions">Updating Distributions<a class="anchor-link" href="#Updating-Distributions">¶</a></h2></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we saw when we studied Bayes' Rule, conditioning gives us a way of updating our opinions based on new data. Let's see how conditional distributions can be used to represent our opinion about a random parameter, based on data.</p>
<h3 id="Example:-A-Randomly-Picked-Coin">Example: A Randomly Picked Coin<a class="anchor-link" href="#Example:-A-Randomly-Picked-Coin">¶</a></h3><p>Suppose I have one fair coin and three biased coins. Suppose that each of the biased coins lands heads with chance 0.9. I pick one of the coins at random, toss it twice and tell you the number of heads. What can you say about whether the coin was fair or biased?</p>
<p>Before I have told you the number of heads, your <em>prior</em> opinion should be that the chance of the fair coin is 0.25 and the chance that the coin is biased is 0.75.</p>
<p>The goal of this example is to see how the information about the number of heads affects this opinion.</p>
<p>Let $R$ be the probability with which the random coin lands heads. The possible values of $R$ are 0.5 and 0.9, and the prior probability distribution of $R$ is given by the following table.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">coins</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>
<span class="n">prior</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">]</span>
<span class="n">Table</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">(</span><span class="n">coins</span><span class="p">)</span><span class="o">.</span><span class="n">probability</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Value</th> <th>Probability</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>0.5  </td> <td>0.25       </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0.9  </td> <td>0.75       </td>
        </tr>
    </tbody>
</table></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let $H$ be the number of heads in the two tosses. Then the joint distribution of $R$ and $H$ consists of terms of the form</p>
<p>$$
P(R = r, H = h) ~~ \text{where } r \in \{0.5, 0.9\} \text{ and }
h \in \{ 0, 1, 2 \}
$$</p>
<p>There are six such terms. Let's work out a couple of them. By the multiplication rule,</p>
<p>$$
P(R = 0.9, H = 2) = P(R = 0.9)P(H = 2 \mid R = 0.9)
= 0.75 \cdot 0.9^2 = 0.6075
$$</p>
<p>The event $\{H = 1\}$ happens if either there is a head followed by a tail or a tail followed by a head. So</p>
<p>$$
P(R = 0.5, H = 1) = P(R = 0.5)P(H = 1 \mid R = 0.5)
= 0.25 [(0.5 \cdot 0.5) + (0.5 \cdot 0.5)] = 0.125
$$</p>
<p>We can use the same kind of reasoning to work out $P(R = r, H = h)$ for all values of $r$ and $h$.</p>
<p>Let's do that directly in Python. The function <code>joint_probs</code> takes $r$ and $h$ as arguments and returns $P(R = r, H = h)$.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">joint_probs</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="sd">"""Return P(R = r, H = h)"""</span>
    
    <span class="c1"># Start with the distribution of the number of heads in two tosses</span>
    <span class="c1"># of a coin that lands heads with a known chance r;</span>
    <span class="c1"># these are the chances of h=0, h=1, and h=2</span>
    
    <span class="n">heads_2_tosses</span> <span class="o">=</span> <span class="n">make_array</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">r</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">r</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">r</span><span class="p">),</span> <span class="n">r</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">r</span> <span class="o">==</span> <span class="mf">0.5</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.25</span><span class="o">*</span><span class="n">heads_2_tosses</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">r</span> <span class="o">==</span> <span class="mf">0.9</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.75</span><span class="o">*</span><span class="n">heads_2_tosses</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can now use <code>prob140</code> methods to construct a joint distribution table for $R$ and $H$, as described in the section on Joint Distributions. Recall that we used <code>coins</code> and <code>prior</code> to construct the prior distribution of $R$:</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">coins</span><span class="p">,</span> <span class="n">prior</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>([0.5, 0.9], [0.25, 0.75])</pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">heads</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">joint_tbl</span> <span class="o">=</span> <span class="n">Table</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">(</span><span class="s1">'R'</span><span class="p">,</span> <span class="n">coins</span><span class="p">,</span> <span class="s1">'H'</span><span class="p">,</span> <span class="n">heads</span><span class="p">)</span><span class="o">.</span><span class="n">probability_function</span><span class="p">(</span><span class="n">joint_probs</span><span class="p">)</span>
<span class="n">joint_dist</span> <span class="o">=</span> <span class="n">joint_tbl</span><span class="o">.</span><span class="n">to_joint</span><span class="p">()</span>

<span class="n">joint_dist</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>R=0.5</th>
      <th>R=0.9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>H=2</th>
      <td>0.0625</td>
      <td>0.6075</td>
    </tr>
    <tr>
      <th>H=1</th>
      <td>0.1250</td>
      <td>0.1350</td>
    </tr>
    <tr>
      <th>H=0</th>
      <td>0.0625</td>
      <td>0.0075</td>
    </tr>
  </tbody>
</table></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The values of $P(R = 0.9, H = 2)$ and $P(R = 0.5, H = 1)$ agree with those that we calculated directly.</p>
<p>Let's check that the marginal of $R$ agrees with the assumption that the coin is picked at random from one fair coin and three biased coins. With no information about the number of heads, the distribution of $R$ should just be the prior. And it is, as you can see in the bottom margin of the table below.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">joint_dist</span><span class="o">.</span><span class="n">marginal</span><span class="p">(</span><span class="s1">'R'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>R=0.5</th>
      <th>R=0.9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>H=2</th>
      <td>0.0625</td>
      <td>0.6075</td>
    </tr>
    <tr>
      <th>H=1</th>
      <td>0.1250</td>
      <td>0.1350</td>
    </tr>
    <tr>
      <th>H=0</th>
      <td>0.0625</td>
      <td>0.0075</td>
    </tr>
    <tr>
      <th>Sum: Marginal of R</th>
      <td>0.2500</td>
      <td>0.7500</td>
    </tr>
  </tbody>
</table></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now suppose I pick the coin (in secret), toss it twice, and tell you the number of heads. Given the value of $H$, what can you say about $R$?</p>
<p>To start off, it's a good idea to find the conditional distribution of $R$ given the value of $H$. Here are all those conditional distributions, for different given values of $H$.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">joint_dist</span><span class="o">.</span><span class="n">conditional_dist</span><span class="p">(</span><span class="s1">'R'</span><span class="p">,</span> <span class="s1">'H'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>R=0.5</th>
      <th>R=0.9</th>
      <th>Sum</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Dist. of R | H=2</th>
      <td>0.093284</td>
      <td>0.906716</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>Dist. of R | H=1</th>
      <td>0.480769</td>
      <td>0.519231</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>Dist. of R | H=0</th>
      <td>0.892857</td>
      <td>0.107143</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>Marginal of R</th>
      <td>0.250000</td>
      <td>0.750000</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Read this table from the bottom row upwards. Remember that the coin was randomly picked.</p>
<ul>
<li>If I tell you nothing about the number of heads, all you can tell me about the coin is that there is 25% chance that it is fair and 75% chance that it is a biased coin.</li>
<li>If I tell you that I got 0 heads, your opinion about the coin changes dramatically in favor of the fair coin. The biased coins have a very small chance of producing no heads, so even though one of them was very likely to have been picked, the data tilt the balance in favor of the fair coin.</li>
<li>If I tell you that I got 1 head, you're in a bit of a quandary. The biased coins have a modest chance (18%) of producing 1 head compared to the 50% chance that the fair coin produces 1 head. But the coin picked had a 75% chance of being biased compared to a 25% chance of being fair. The size of these two effects makes you quite uncertain about which kind of coin to lean towards. You have only a slight lean towards 'biased'.</li>
<li>If I tell you that I got 2 heads, your opinion shifts dramatically towards the biased coins. Not only are they very likely to produce two heads, they are also very likely to have been picked.</li>
</ul></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Updating-Your-Opinion-Based-on-Data">Updating Your Opinion Based on Data<a class="anchor-link" href="#Updating-Your-Opinion-Based-on-Data">¶</a></h3><p>This is a simple example of something that comes up often in machine learning.</p>
<ul>
<li>You start out with a <em>prior</em> opinion about an unknown quantity. In our case the prior distribution was that the fair coin would be picked with chance 25%.</li>
<li>For every value of the unknown quantity, the data have a <em>likelihood</em>. For each of our four coins, we know the likelihood of getting any specified number of heads given that we tossed that coin.</li>
<li>After you see the data, your opinion about the unknown quantity might change, sometimes quite drastically. The change depends on the prior as well as the likelihood.</li>
<li>You can then toss some more, and update your opinion once again based on the number of heads in the new set of tosses.</li>
</ul></div></div></div>