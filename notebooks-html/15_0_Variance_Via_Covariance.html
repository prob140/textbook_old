<div id="ipython-notebook">
            <a class="interact-button" href="http://prob140.berkeley.edu/user-redirect/interact?repo=prob140&path=textbook/Chapter 15/15_0_Variance_Via_Covariance.ipynb">Interact</a>
            
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Variance-Via-Covariance">Variance Via Covariance<a class="anchor-link" href="#Variance-Via-Covariance">¶</a></h1><p>In this chapter we return to random sampling, in particular to the variability in the sum of a random sample. Binomial and hypergeometric random variables are such sums. Means of random samples are easily calculated once we have the sample sum. So it is worth taking some time to understand how sample sums behave.</p>
<p>By Chebychev's inequality, the value of a random variable $X$ is most likely to be in the range "$E(X) \pm$ a few $SD(X)$". The measure of spread $SD(X)$ is the root mean squared deviation of $X$ from the mean:</p>
$$
SD(X) = \sqrt{E[(X-E(X))^2]}
$$<p>Variance is the square of the standard deviation. We know that variance has better computational properties than the SD, so this chapter will focus on ways to find variance. We will use some familiar shorthand:</p>
<ul>
<li>$\mu_X = E(X)$</li>
<li>$\sigma_X = SD(X)$</li>
</ul>
<p>Let $D_X = X - \mu_X$ denote the deviation of $X$ from its mean. Then</p>
$$
Var(X) = \sigma_X^2 = E(D_X^2)
$$</div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Variance-of-a-Sum">Variance of a Sum<a class="anchor-link" href="#Variance-of-a-Sum">¶</a></h3><p>Let $X$ and $Y$ be two random variables on the same space, and let $S = X+Y$. Then $E(S) = \mu_X + \mu_Y$, and the deviation of $S$ is the sum of the deviations of $X$ and $Y$:</p>
$$
D_S ~ = ~ S - \mu_S ~ = ~ X + Y - (\mu_X + \mu_Y) ~ = ~ D_X + D_Y
$$<p>This gives us some insight into the variance of the sum $S$.</p>
\begin{align*}
Var(S) &amp;= E(D_S^2) \\
&amp;= E[(D_X + D_Y)^2] \\
&amp;= E(D_X^2) + E(D_Y^2) + 2E(D_XD_Y) \\
&amp;= Var(X) + Var(Y) + 2E(D_XD_Y)
\end{align*}<p>The first thing to note is that while the expectation of a sum is the sum of the expectations, the calculation above shows that the variance of a sum is in general <strong>not</strong> the sum of the variances. There's an extra term.</p>
<p>To calculate the variance of a sum, we have to understand that extra term.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Covariance">Covariance<a class="anchor-link" href="#Covariance">¶</a></h3><p>The <em>covariance of $X$ and $Y$</em>, denoted $Cov(X, Y)$, is the expected product of the deviations of $X$ and $Y$:</p>
$$
Cov(X, Y) ~ = ~ E(D_XD_Y) ~=~ E[(X - \mu_X)(Y - \mu_Y)]
$$</div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this chapter we will learn how to utilize covariance to find variances of sums. The fundamental calculation is the one we did above; here is the result again, using the language of covariance.</p>
$$
Var(X+Y) ~ = ~ Var(X) + Var(Y) + 2Cov(X, Y)
$$</div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div></div></div></div>